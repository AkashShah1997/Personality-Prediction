{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_gHgP9WqjB1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "import random\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "import os\n",
        "from keras.models import load_model\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "BY-P2UrzqpLt",
        "outputId": "90bf68e4-aa27-4d5d-b4a2-10f8f245943c"
      },
      "outputs": [],
      "source": [
        "# Read in the table to pandas dataframe\n",
        "original_data = pd.read_csv('mbti_1.csv')\n",
        "original_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "L2nCAdS0q7i7",
        "outputId": "da7e2c98-6f52-4fcf-c6cc-443cee8f794d"
      },
      "outputs": [],
      "source": [
        "# Change column names\n",
        "original_data.columns = ['type','posts']\n",
        "original_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1h7-1oT4q-hV"
      },
      "outputs": [],
      "source": [
        "# Analyze the correlation between different personality types\n",
        "split_data = original_data[['type']].copy()\n",
        "\n",
        "split_data['E-I'] = original_data['type'].str.extract('(.)[N,S]',1)\n",
        "split_data['N-S'] = original_data['type'].str.extract('[E,I](.)[F,T]',1)\n",
        "split_data['T-F'] = original_data['type'].str.extract('[N,S](.)[J,P]',1)\n",
        "split_data['J-P'] = original_data['type'].str.extract('[F,T](.)',1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "OfvJx3YMZnCP",
        "outputId": "d94a1219-32eb-4701-db6d-d0b53b4ebc39"
      },
      "outputs": [],
      "source": [
        "# Encode letters to numeric values\n",
        "le = LabelEncoder()\n",
        "\n",
        "encoded_data = split_data[['type']].copy()\n",
        "encoded_data['E0-I1'] = le.fit_transform(split_data['E-I'])\n",
        "encoded_data['N0-S1'] = le.fit_transform(split_data['N-S'])\n",
        "encoded_data['F0-T1'] = le.fit_transform(split_data['T-F'])\n",
        "encoded_data['J0-P1'] = le.fit_transform(split_data['J-P'])\n",
        "\n",
        "corr_data = encoded_data.drop(columns='type')\n",
        "\n",
        "correlation = corr_data.corr()\n",
        "correlation.style.background_gradient()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "M7UeVt1trC9M",
        "outputId": "2ab59bcc-376c-48c2-d61d-fe300ec031c6"
      },
      "outputs": [],
      "source": [
        "# Analyze the personality type distribution\n",
        "count_person_types = original_data.groupby('type').agg({'type':'count'})\n",
        "count_person_types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "44e8GZNvrFwd",
        "outputId": "524b1eeb-1f00-493f-b7c3-4ef23d504f16"
      },
      "outputs": [],
      "source": [
        "# Create a bar chart based off of the group series from before\n",
        "count_person_chart = count_person_types.plot(kind='bar')\n",
        "count_person_chart.set_xlabel(\"Personality Types\")\n",
        "count_person_chart.set_ylabel(\"Number of Samples\")\n",
        "\n",
        "plt.show()\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "55Zcqu5orIXe",
        "outputId": "0490fa5e-effa-4963-f1f3-1d8d999b4f9e"
      },
      "outputs": [],
      "source": [
        "# Count personality type combination\n",
        "split_data.groupby('E-I').agg({'E-I':'count'}).plot(kind='bar')\n",
        "split_data.groupby('N-S').agg({'N-S':'count'}).plot(kind='bar')\n",
        "split_data.groupby('T-F').agg({'type':'count'}).plot(kind='bar')\n",
        "split_data.groupby('J-P').agg({'type':'count'}).plot(kind='bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "8C-_zJowrLVc",
        "outputId": "ff3c5291-1fff-4388-a1c8-e023f68626a8"
      },
      "outputs": [],
      "source": [
        "# count 'http'\n",
        "original_data_copy = original_data.copy()\n",
        "original_data_copy['http_per_post']=original_data['posts'].apply(lambda x: x.count('http')/50)\n",
        "original_data_copy.head()\n",
        "\n",
        "# create a new field without http sting\n",
        "p = \"(http.*?\\s)\"\n",
        "original_data['no_url']=original_data['posts'].replace(p,\" \",regex=True)\n",
        "original_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "QTnU5Sn_rPY4",
        "outputId": "15abace6-2e44-44b3-a49e-e53ce5a8a0b4"
      },
      "outputs": [],
      "source": [
        "# count question marks\n",
        "original_data_copy['?_per_post']=original_data['no_url'].apply(lambda x: x.count('?')/50)\n",
        "original_data_copy.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "C63zf32NrRUn",
        "outputId": "0e67f67a-4df6-4f96-8b2b-17410d0fbc43"
      },
      "outputs": [],
      "source": [
        "# count exclamation marks\n",
        "original_data_copy['!_per_post']=original_data['no_url'].apply(lambda x: x.count('!')/50)\n",
        "original_data_copy.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6as528FLrTqg",
        "outputId": "f25fb152-5baf-407a-cac6-fe77fcaaa50c"
      },
      "outputs": [],
      "source": [
        "# create a column without ||| sting\n",
        "p = \"(\\|\\|\\|)\"\n",
        "original_data['text']=original_data['no_url'].replace(p,\" \",regex=True)\n",
        "original_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5_r60ZptrXf8",
        "outputId": "f77d8762-ce6a-42ef-c639-dafe71295e7d"
      },
      "outputs": [],
      "source": [
        "# Count the length of each post\n",
        "original_data_copy['length_per_post'] = original_data['text'].apply(lambda x: len(x)/50)\n",
        "original_data_copy.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "20ZpiLP6rZ6U",
        "outputId": "5ebfa23f-1d81-4aaf-dad8-4a9504f80188"
      },
      "outputs": [],
      "source": [
        "# count digits\n",
        "original_data_copy['digits_per_post'] = original_data['text'].apply(lambda x: sum(c.isdigit() for c in x)/50)\n",
        "original_data_copy.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "2UY0cjUWrb8n",
        "outputId": "4008da27-43a4-4b11-fd47-1a1a7b805ba5"
      },
      "outputs": [],
      "source": [
        "# Group the counts by type\n",
        "analysis_group = original_data_copy.groupby('type').mean()\n",
        "analysis_group.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w-MkiusYre8W",
        "outputId": "adf6c87e-825f-495d-d509-9f8a20cf6b2d"
      },
      "outputs": [],
      "source": [
        "# Plot the counts in a grouped bar chart\n",
        "analysis_group.plot(kind='bar', subplots=True, title=\"Writing analysis (per post)\",\n",
        "        layout=(5, 1), sharex=True, sharey=False, legend=False,\n",
        "              figsize=(8,12),rot=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Zaj_T1U2rkNW",
        "outputId": "4b33ada2-91a3-41e8-fb47-0e96498aa18d"
      },
      "outputs": [],
      "source": [
        "# remove all punctuations\n",
        "p = \"[^\\w\\s]\"\n",
        "original_data['text']=original_data['text'].replace(p,\" \",regex=True)\n",
        "original_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "VaQlF7fprl6J",
        "outputId": "2c3a7b6e-be90-4f52-db04-16ba46e160f5"
      },
      "outputs": [],
      "source": [
        "# remove underscore\n",
        "p = \"\\_\"\n",
        "original_data['text']=original_data['text'].replace(p,\" \",regex=True)\n",
        "original_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "iiApoFAArn_5",
        "outputId": "7312b1cd-00f7-4043-de02-7723927f6810"
      },
      "outputs": [],
      "source": [
        "# remove all numbers\n",
        "p = \"\\d+\"\n",
        "original_data['text']=original_data['text'].replace(p,\" \",regex=True)\n",
        "original_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "mecBtaJtrqX6",
        "outputId": "0f379c97-9870-49f9-83a8-03524ed7e031"
      },
      "outputs": [],
      "source": [
        "# remove one letter words\n",
        "p = \"\\W*\\b\\w\\b\"\n",
        "original_data['text']=original_data['text'].replace(p,\" \",regex=True)\n",
        "original_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ULMv-H4Lrswt",
        "outputId": "194952c9-fb3d-43fd-9f4f-05f3b424a78c"
      },
      "outputs": [],
      "source": [
        "# make everything lowercase\n",
        "original_data['text'] = original_data['text'].str.lower()\n",
        "original_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "jFBqO_qmrvvQ",
        "outputId": "df99c954-6056-4100-b499-4c9d0904c855"
      },
      "outputs": [],
      "source": [
        "# save the cleaned df\n",
        "cleaned_data = original_data[['type','text']]\n",
        "cleaned_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCmjJZzrvnx_"
      },
      "outputs": [],
      "source": [
        "# Filter rows containing the keyword in any column\n",
        "filtered_data = cleaned_data[cleaned_data.apply(lambda text: 'ESTJ' in text.values, axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaM2gRipvnx_",
        "outputId": "7fceaaa5-81d2-4abe-fbb5-bc1e8e5b59ad"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize words\n",
        "word_lists = [word_tokenize(text) for text in filtered_data['text']]\n",
        "\n",
        "# Flatten the list of lists into a single list\n",
        "words = [word for word_list in word_lists for word in word_list]\n",
        "\n",
        "# Create a new dataframe with individual words\n",
        "word_data = pd.DataFrame({'word': words})\n",
        "\n",
        "print(word_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0d9OkoVvnx_",
        "outputId": "196c5afe-ae3f-4712-c068-191144a28961"
      },
      "outputs": [],
      "source": [
        "unique_words = word_data['word'].unique()\n",
        "\n",
        "print('Number of unique words: ', len(unique_words))\n",
        "\n",
        "# Choose 7000 random words from the unique words list.\n",
        "# This is done because there is only 5881 unique words in the dataset and this will help us to generate sentences with some repeated words.\n",
        "random_words = random.choices(unique_words, k=7000)\n",
        "\n",
        "# Create sentences of 7000 words each (448 sentences)\n",
        "num_sentences = 4000\n",
        "words_per_sentence = 448\n",
        "\n",
        "sentences = []\n",
        "for _ in range(num_sentences):\n",
        "    sentence = ' '.join(random.sample(random_words, words_per_sentence))\n",
        "    sentences.append(sentence)\n",
        "\n",
        "# Create a new dataframe with the generated sentences\n",
        "new_data = {'sentence': sentences}\n",
        "new_df = pd.DataFrame(new_data)\n",
        "\n",
        "print(new_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyKvaIwPvnx_",
        "outputId": "84e608fc-7595-4fa9-d493-3a3790202a28"
      },
      "outputs": [],
      "source": [
        "# Add the 'type' column with 'ESTJ' value at the beginning\n",
        "new_df.insert(0, 'type', 'ESTJ')\n",
        "\n",
        "# Rename the 'sentence' column to 'text'\n",
        "new_df.rename(columns={'sentence': 'text'}, inplace=True)\n",
        "\n",
        "print(new_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zPa4ClMvnx_",
        "outputId": "f3856324-8078-4bcb-b47b-c0f0804c9062"
      },
      "outputs": [],
      "source": [
        "# Concatenate the dataframes along the rows\n",
        "merged_df = pd.concat([cleaned_data, new_df], ignore_index=True)\n",
        "\n",
        "print(merged_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MWytY0ggvnx_",
        "outputId": "620642cf-e84f-4a56-87f0-18d82718cdf6"
      },
      "outputs": [],
      "source": [
        "# Count personality type combination\n",
        "\n",
        "split_data1 = merged_df[['type']].copy()\n",
        "\n",
        "split_data1['E-I'] = merged_df['type'].str.extract('(.)[N,S]',1)\n",
        "split_data1['N-S'] = merged_df['type'].str.extract('[E,I](.)[F,T]',1)\n",
        "split_data1['T-F'] = merged_df['type'].str.extract('[N,S](.)[J,P]',1)\n",
        "split_data1['J-P'] = merged_df['type'].str.extract('[F,T](.)',1)\n",
        "\n",
        "le1 = LabelEncoder()\n",
        "\n",
        "encoded_data1 = split_data1[['type']].copy()\n",
        "encoded_data1['E0-I1'] = le1.fit_transform(split_data1['E-I'])\n",
        "encoded_data1['N0-S1'] = le1.fit_transform(split_data1['N-S'])\n",
        "encoded_data1['F0-T1'] = le1.fit_transform(split_data1['T-F'])\n",
        "encoded_data1['J0-P1'] = le1.fit_transform(split_data1['J-P'])\n",
        "\n",
        "split_data1.groupby('E-I').agg({'E-I':'count'}).plot(kind='bar')\n",
        "split_data1.groupby('N-S').agg({'N-S':'count'}).plot(kind='bar')\n",
        "split_data1.groupby('T-F').agg({'type':'count'}).plot(kind='bar')\n",
        "split_data1.groupby('J-P').agg({'type':'count'}).plot(kind='bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XImRtenyvnx_",
        "outputId": "4e3e0eba-ed81-47ee-bed5-ede5dacad035"
      },
      "outputs": [],
      "source": [
        "# Generate word clouds for each personality type\n",
        "def generate_word_cloud(posts, personality_type):\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(posts)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Word Cloud for {personality_type} Personality Type')\n",
        "    plt.show()\n",
        "\n",
        "# Group data by personality type\n",
        "grouped_data = merged_df.groupby('type')\n",
        "\n",
        "# Generate word clouds for each personality type\n",
        "for personality_type, group in grouped_data:\n",
        "    posts_combined = ' '.join(group['text'])\n",
        "    generate_word_cloud(posts_combined, personality_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "DGfyGUfprxu_",
        "outputId": "02919b5b-52d6-4bc1-96cb-ff24f06a30f4"
      },
      "outputs": [],
      "source": [
        "# Split type columns into four binary columns\n",
        "split_data = merged_df[['type','text']].copy()\n",
        "split_data['E-I'] = split_data['type'].str.extract('(.)[N,S]',1)\n",
        "split_data['N-S'] = split_data['type'].str.extract('[E,I](.)[F,T]',1)\n",
        "split_data['T-F'] = split_data['type'].str.extract('[N,S](.)[J,P]',1)\n",
        "split_data['J-P'] = split_data['type'].str.extract('[F,T](.)',1)\n",
        "split_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "6BDWS-Opr0Il",
        "outputId": "ea8fedf1-a684-4b9d-8af0-8f06be8791d5"
      },
      "outputs": [],
      "source": [
        "# Encode letters to numeric values\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "encoded_data = merged_df[['type','text']].copy()\n",
        "encoded_data['E0-I1'] = le.fit_transform(split_data['E-I'])\n",
        "encoded_data['N0-S1'] = le.fit_transform(split_data['N-S'])\n",
        "encoded_data['F0-T1'] = le.fit_transform(split_data['T-F'])\n",
        "encoded_data['J0-P1'] = le.fit_transform(split_data['J-P'])\n",
        "\n",
        "encoded_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMvzWEiSsAsj",
        "outputId": "02eccb15-0521-46f4-9f26-96695ccfb3e7"
      },
      "outputs": [],
      "source": [
        "encoded_data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIwOMSN5sEr8"
      },
      "outputs": [],
      "source": [
        "# Define X and y\n",
        "X = encoded_data[\"text\"].values\n",
        "y_all = encoded_data.drop(columns=['type', 'text'])\n",
        "\n",
        "# Split training and testing dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_all_train, y_all_test = train_test_split(X, y_all, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCI2UoU8sHJZ"
      },
      "outputs": [],
      "source": [
        "# Define TFIDF verctorizer\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=17000,\n",
        "    min_df=7,\n",
        "    max_df=0.8,\n",
        "    stop_words=\"english\",\n",
        "    ngram_range=(1,3),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaz0hDehsJ5p"
      },
      "outputs": [],
      "source": [
        "# create vectors for X\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miI8H0cP19xn",
        "outputId": "fc1ae9ed-7d63-4f30-b794-0dc1f5935aba"
      },
      "outputs": [],
      "source": [
        "# Assuming you've already transformed your text data into TF-IDF vectors\n",
        "# X_train, X_test, y_all_train, y_all_test\n",
        "\n",
        "# Define the combinations\n",
        "combinations = ['E0-I1', 'N0-S1', 'F0-T1', 'J0-P1']\n",
        "\n",
        "# Create a dictionary to hold models\n",
        "models = {}\n",
        "\n",
        "# Loop through combinations\n",
        "for combination in combinations:\n",
        "    y_combination_train = y_all_train[combination]\n",
        "    y_combination_test = y_all_test[combination]\n",
        "\n",
        "    # Create the RNN model\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=len(vectorizer.get_feature_names_out()), output_dim=128))\n",
        "    model.add(LSTM(64, return_sequences=True))\n",
        "    model.add(LSTM(32, return_sequences=False))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train.toarray(), y_combination_train, epochs=3, batch_size=32, validation_split=0.2)\n",
        "\n",
        "    # Store the model\n",
        "    models[combination] = model\n",
        "\n",
        "# Evaluate models if needed\n",
        "for combination, model in models.items():\n",
        "    loss, accuracy = model.evaluate(X_test.toarray(), y_all_test[combination])\n",
        "    print(f\"Combination: {combination} - Test Loss: {loss} - Test Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAB89QIQv24j"
      },
      "outputs": [],
      "source": [
        "# Create the \"model\" folder if it doesn't exist\n",
        "if not os.path.exists('model'):\n",
        "    os.makedirs('model')\n",
        "\n",
        "# Save each model in the \"model\" folder\n",
        "for combination, model in models.items():\n",
        "    model.save(f'model/{combination}.h5')\n",
        "\n",
        "# Save the vectorizer in the \"model\" folder\n",
        "with open('model/vectorizer.pkl', 'wb') as f:\n",
        "    pickle.dump(vectorizer, f)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
